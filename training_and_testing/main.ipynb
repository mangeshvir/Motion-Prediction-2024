{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b289c30b-fa3a-4e4b-94be-ad19b1fefa92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Import Statements and Settings\n",
    "import logging\n",
    "import lightning as pl\n",
    "import torch\n",
    "import wandb\n",
    "from callbacks import create_callbacks\n",
    "from lit_datamodule import inD_RecordingModule\n",
    "from lit_module import LitModule\n",
    "from utils import create_wandb_logger, get_data_path, build_module\n",
    "from nn_modules import ConstantVelocityModel, MultiLayerPerceptron, LSTMModel\n",
    "from select_features import select_features\n",
    "\n",
    "torch.set_float32_matmul_precision('medium')\n",
    "torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d4aefa0-5436-49e4-a4cd-cb9d3a5d10be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Initialization\n",
    "data_path, log_path = get_data_path()\n",
    "wandb.login()\n",
    "\n",
    "project_name = \"SS2024_motion_prediction\"\n",
    "stage = input(\"Please enter the stage (fit or test): \").strip().lower()\n",
    "recording_ID = 26"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fea4f0d-ff6e-41dd-9725-e2e49f041d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Feature Selection\n",
    "features, number_of_features = select_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2da16bc-0c05-4dd9-bb24-1e34a3132c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common Parameters\n",
    "past_sequence_length = 6\n",
    "future_sequence_length = 3\n",
    "sequence_length = past_sequence_length + future_sequence_length\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c6449f-ea10-4ae8-9df5-70dfbef0e7ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Model Selection\n",
    "print(\"Please select the model you wish to use:\")\n",
    "print(\"1: Multi-Layer Perceptron (MLP)\")\n",
    "print(\"2: Long Short-Term Memory (LSTM)\")\n",
    "print(\"3: Constant Velocity Model\")\n",
    "print(\"4: Constant Acceleration Model\")\n",
    "model_choice = int(input(\"Enter the number corresponding to your model choice: \").strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e77ee74-b2bc-46b3-95d3-7c1e0631b235",
   "metadata": {},
   "outputs": [],
   "source": [
    "if model_choice == 1:\n",
    "    # MLP Model Configuration\n",
    "    input_size = number_of_features * past_sequence_length\n",
    "    output_size = number_of_features\n",
    "    hidden_size = 32\n",
    "    epochs = 3\n",
    "    \n",
    "    mdl = MultiLayerPerceptron(input_size, hidden_size, output_size)\n",
    "    \n",
    "    dm = inD_RecordingModule(\n",
    "        data_path, recording_ID, sequence_length, past_sequence_length, \n",
    "        future_sequence_length, features, batch_size=batch_size\n",
    "    )\n",
    "    \n",
    "    model_params = {\n",
    "        'model': mdl,\n",
    "        'number_of_features': number_of_features,\n",
    "        'sequence_length': sequence_length,\n",
    "        'past_sequence_length': past_sequence_length,\n",
    "        'future_sequence_length': future_sequence_length,\n",
    "        'batch_size': batch_size\n",
    "    }\n",
    "    model = LitModule(**model_params)\n",
    "\n",
    "    # Data Module Setup\n",
    "    dm.setup(stage=stage)\n",
    "\n",
    "    # Log Path\n",
    "    print(log_path)\n",
    "\n",
    "    # Callbacks and WandB Logger\n",
    "    callbacks = create_callbacks()\n",
    "    wandb_logger = create_wandb_logger(log_path, project_name, recording_ID)\n",
    "    wandb_logger.experiment.config.update({\n",
    "        \"batch_size\": batch_size,\n",
    "        \"sequence_length\": sequence_length\n",
    "    })\n",
    "    logging.getLogger(f\"{log_path}lightning\").setLevel(logging.ERROR)\n",
    "\n",
    "    # Trainer Setup\n",
    "    trainer = pl.Trainer(max_epochs=epochs,\n",
    "                         fast_dev_run=False,\n",
    "                         devices=\"auto\",\n",
    "                         accelerator=\"auto\",\n",
    "                         log_every_n_steps=5,\n",
    "                         logger=wandb_logger,\n",
    "                         callbacks=callbacks,\n",
    "                         check_val_every_n_epoch=1,\n",
    "                         precision=32)\n",
    "\n",
    "    # Training or Testing\n",
    "    if stage == \"fit\":\n",
    "        trainer.fit(model, dm)\n",
    "    elif stage == \"test\":\n",
    "        checkpoint_path = input(\"Please enter the checkpoint path: \").strip()\n",
    "        model = LitModule.load_from_checkpoint(checkpoint_path, **model_params)\n",
    "        trainer.test(model, dm)\n",
    "        wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7532c9-24e1-4198-9abb-5540dd734e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "if model_choice == 2:\n",
    "    # LSTM Model Configuration\n",
    "    input_size = number_of_features\n",
    "    output_size = number_of_features  # Assuming output is a 2D coordinate\n",
    "    hidden_size = 64\n",
    "    num_layers = 2\n",
    "    epochs = 3\n",
    "    mdl = LSTMModel(input_size, hidden_size, num_layers, output_size, future_sequence_length)\n",
    "    \n",
    "    dm = inD_RecordingModule(\n",
    "        data_path, recording_ID, sequence_length, past_sequence_length, \n",
    "        future_sequence_length, features, batch_size=batch_size, \n",
    "        use_lstm=True\n",
    "    )\n",
    "    \n",
    "    model_params = {\n",
    "        'model': mdl,\n",
    "        'number_of_features': number_of_features,\n",
    "        'sequence_length': sequence_length,\n",
    "        'past_sequence_length': past_sequence_length,\n",
    "        'future_sequence_length': future_sequence_length,\n",
    "        'batch_size': batch_size,\n",
    "        'use_lstm': True\n",
    "    }\n",
    "    model = LitModule(**model_params)\n",
    "\n",
    "    # Data Module Setup\n",
    "    dm.setup(stage=stage)\n",
    "\n",
    "    # Log Path\n",
    "    print(log_path)\n",
    "\n",
    "    # Callbacks and WandB Logger\n",
    "    callbacks = create_callbacks()\n",
    "    wandb_logger = create_wandb_logger(log_path, project_name, recording_ID)\n",
    "    wandb_logger.experiment.config.update({\n",
    "        \"batch_size\": batch_size,\n",
    "        \"sequence_length\": sequence_length\n",
    "    })\n",
    "    logging.getLogger(f\"{log_path}lightning\").setLevel(logging.ERROR)\n",
    "\n",
    "    # Trainer Setup\n",
    "    trainer = pl.Trainer(max_epochs=epochs,\n",
    "                         fast_dev_run=False,\n",
    "                         devices=\"auto\",\n",
    "                         accelerator=\"auto\",\n",
    "                         log_every_n_steps=5,\n",
    "                         logger=wandb_logger,\n",
    "                         callbacks=callbacks,\n",
    "                         check_val_every_n_epoch=1,\n",
    "                         precision=32)\n",
    "\n",
    "    # Training or Testing\n",
    "    if stage == \"fit\":\n",
    "        trainer.fit(model, dm)\n",
    "    elif stage == \"test\":\n",
    "        checkpoint_path = input(\"Please enter the checkpoint path: \").strip()\n",
    "        model = LitModule.load_from_checkpoint(checkpoint_path, **model_params)\n",
    "        trainer.test(model, dm)\n",
    "        wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd8c17f-f6af-4c61-87d8-0b1116b99b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "if model_choice == 3:\n",
    "    # Constant Velocity Model Configuration\n",
    "    mdl = ConstantVelocityModel()\n",
    "    \n",
    "    dm = inD_RecordingModule(\n",
    "        data_path, recording_ID, sequence_length, past_sequence_length, \n",
    "        future_sequence_length, features, batch_size=batch_size\n",
    "    )\n",
    "    \n",
    "    model_params = {\n",
    "        'model': mdl,\n",
    "        'number_of_features': number_of_features,\n",
    "        'sequence_length': sequence_length,\n",
    "        'past_sequence_length': past_sequence_length,\n",
    "        'future_sequence_length': future_sequence_length,\n",
    "        'batch_size': batch_size\n",
    "    }\n",
    "    model = LitModule(**model_params)\n",
    "\n",
    "    # Data Module Setup\n",
    "    dm.setup(stage=stage)\n",
    "\n",
    "    # Log Path\n",
    "    print(log_path)\n",
    "\n",
    "    # Testing\n",
    "    if stage == \"test\":\n",
    "        # Direct prediction using Constant Velocity Model\n",
    "        test_dataloader = dm.test_dataloader()\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for batch in test_dataloader:\n",
    "                x, y = batch\n",
    "                y_hat = model(x)\n",
    "                loss = model.loss_function(y_hat, y)\n",
    "                print(f\"Test loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f25634bb-f2c7-428e-831b-a709ab0603c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if model_choice == 4:\n",
    "    # Constant Acceleration Model Configuration\n",
    "    class ConstantAccelerationModel(torch.nn.Module):\n",
    "        def __init__(self, dt=1.0):\n",
    "            super(ConstantAccelerationModel, self).__init__()\n",
    "            self.dt = dt\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = x[:, -1, :]\n",
    "            return x + self.dt * x + 0.5 * (self.dt ** 2) * x\n",
    "\n",
    "        def loss_function(self, y_hat, y):\n",
    "            return torch.nn.functional.mse_loss(y_hat, y)\n",
    "    \n",
    "    mdl = ConstantAccelerationModel()\n",
    "    \n",
    "    dm = inD_RecordingModule(\n",
    "        data_path, recording_ID, sequence_length, past_sequence_length, \n",
    "        future_sequence_length, features, batch_size=batch_size\n",
    "    )\n",
    "    \n",
    "    model_params = {\n",
    "        'model': mdl,\n",
    "        'number_of_features': number_of_features,\n",
    "        'sequence_length': sequence_length,\n",
    "        'past_sequence_length': past_sequence_length,\n",
    "        'future_sequence_length': future_sequence_length,\n",
    "        'batch_size': batch_size\n",
    "    }\n",
    "    model = LitModule(**model_params)\n",
    "\n",
    "    # Data Module Setup\n",
    "    dm.setup(stage=stage)\n",
    "\n",
    "    # Log Path\n",
    "    print(log_path)\n",
    "\n",
    "    # Testing\n",
    "    if stage == \"test\":\n",
    "        # Direct prediction using Constant Acceleration Model\n",
    "        test_dataloader = dm.test_dataloader()\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for batch in test_dataloader:\n",
    "                x, y = batch\n",
    "                y_hat = model(x)\n",
    "                loss = model.loss_function(y_hat, y)\n",
    "                print(f\"Test loss: {loss.item()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
